#+TITLE: GradientZ 
#+OPTIONS: toc:nil
#+ATTR_HTML: :align right
#+ATTR_ORG: :align center
* GradientZ
#+ATTR_HTML: :align right
#+ATTR_ORG: :align center
[[file:assets/Logo.png]]


A autograd engine implementation inspired by [[https://github.com/karpathy/micrograd][micrograd]], but with tensor support written in Zig.
This library is a toy project made for educational purposes, to learn more about Zig and how to implement a tensor library with autograd capabilities.

* Current Features
*** Tensor Creation and Management
+ [X] Multi-dimensional tensor support (0D, 1D, 2D, 3D, N-D)
+ [X] Memory-efficient row-major layout
+ [X] CPU backend with full functionality
+ [X] Device abstraction (CPU ready, GPU planned)
+ [X] Shape validation and error handling
+ [X] Tensor visualization and pretty printing

*** Basic Arithmetic Operations (Element-wise)
+ [X] Addition (~add~) - works on all dimensions
+ [X] Subtraction (~sub~) - works on all dimensions
+ [X] Multiplication (~mul~) - element-wise, works on all dimensions
+ [X] Division (~div~) - element-wise with zero checking
+ [X] Power (~pow~) - element-wise exponentiation
+ [X] Negation (~neg~) - element-wise negation

*** Matrix Operations
+ [X] Matrix multiplication (~matmul~)
+ [X] Shape compatibility checking
+ [X] Transpose (~transpose~) - for 2D tensors

*** Utility Functions
+ [X] ~zeros()~ - create zero-filled tensors
+ [X] ~ones()~ - create one-filled tensors
+ [X] ~fill()~ - fill tensor with constant value
+ [X] ~setData()~ - set tensor data from slice

*** Activation Functions
+ [X] ReLU (~relu~) - max(0, x)
+ [X] Sigmoid (~sigmoid~) - 1/(1+exp(-x))
+ [X] Tanh (~tanh~) - hyperbolic tangent
+ [X] Exponential (~exp~) - e^x
+ [X] Natural log (~log~) - ln(x)
+ [X] Square root (~sqrt~) - âˆšx
+ [X] Absolute value (~abs~) - |x|

*** Reduction Operations
+ [X] Sum (~sum~) - sum all elements to scalar
+ [X] Mean (~mean~) - average of all elements
+ [ ] Max (~max~) - maximum element
+ [ ] Min (~min~) - minimum element

*** Advanced Operations
+ [ ] Element-wise maximum (~maximum~) - max(a, b)
+ [X] Sine (~sin~) - sin(x)
+ [X] Cosine (~cos~) - cos(x)

*** Broadcasting Support
+ [ ] Addition with broadcasting (~addBroadcast~)

*** Neural Network Module (PyTorch-like API) needs fixes before beeing used
+ [X] ~nn.Module~ - Base module class (similar to PyTorch's nn.Module)
+ [X] ~nn.Linear~ - Fully connected layer with Xavier initialization and bias
+ [X] ~nn.ReLU~ - ReLU activation function
+ [X] ~nn.Sigmoid~ - Sigmoid activation function
+ [X] ~nn.SimpleMLP~ - Multi-layer perceptron implementation
+ [X] Forward pass implementation for all layers
+ [X] Batch processing support (2D inputs)
+ [X] Automatic gradient computation support
+ [X] Memory management with proper cleanup

* Examples

** Running Examples

#+begin_src bash
# Run individual examples
zig build example-scalars      # 0D tensors (scalars)
zig build example-vectors      # 1D tensors (vectors)
zig build example-matrices     # 2D tensors (matrices)
zig build example-3d           # 3D tensors
zig build example-autograd     # Automatic differentiation
zig build example-mlp          # Multi-Layer Perceptron
zig build example-devices      # Device functionality
zig build example-training     # Training with gradient descent
zig build example              # PyTorch-like API example

# Run all examples
zig build examples
#+end_src

** Quick Demo

#+begin_src zig
const std = @import("std");
const GradientZ = @import("GradientZ_lib");

pub fn main() !void {
    // Initialize library
    GradientZ.init();

    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Create tensors
    var a = try GradientZ.zeros(allocator, &.{2, 3}, GradientZ.cpu());
    defer a.deinit();

    var b = try GradientZ.ones(allocator, &.{2, 3}, GradientZ.cpu());
    defer b.deinit();

    // Basic operations
    var result = try a.add(&b);
    defer result.deinit();
    try result.print();
}
#+end_src

** Example Categories

- *Basic Operations*: [[file:examples/scalars_0d.zig][scalars_0d.zig]], [[file:examples/vectors_1d.zig][vectors_1d.zig]], [[file:examples/matrices_2d.zig][matrices_2d.zig]], [[file:examples/tensors_3d.zig][tensors_3d.zig]]
- *Automatic Differentiation*: [[file:examples/autograd.zig][autograd.zig]]
- *Neural Networks*: [[file:examples/mlp.zig][mlp.zig]], [[file:examples/pytorch_like.zig][pytorch_like.zig]]
- *Training*: [[file:examples/training.zig][training.zig]]
- *Device Management*: [[file:examples/devices.zig][devices.zig]]

